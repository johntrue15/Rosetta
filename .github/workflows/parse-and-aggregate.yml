name: Parse & Aggregate Data

on:
  push:
    # Watch all content under data/**, but ignore JSON to avoid loops
    paths:
      - 'data/**'
    paths-ignore:
      - 'data/**/*.json'
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: parse-and-aggregate-${{ github.ref }}
  cancel-in-progress: false

jobs:
  parse:
    runs-on: ubuntu-latest
    if: "!contains(github.event.head_commit.message, '[skip ci]')"

    steps:
      - name: Check out repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install parser deps
        run: |
          python -m pip install --upgrade pip
          pip install striprtf pandas

      - name: Make sure folders exist
        run: |
          mkdir -p scripts
          mkdir -p data/parsed
          mkdir -p data/completed

      # --- Drop your parser scripts into the runner workspace ---
      - name: Write parser scripts
        shell: bash
        run: |
          cat > scripts/parse_any.py << 'PY'
{{PARSE_ANY_PY}}
PY
          cat > scripts/rtf_to_json.py << 'PY'
{{RTF_TO_JSON_PY}}
PY
          cat > scripts/pca_to_json.py << 'PY'
{{PCA_TO_JSON_PY}}
PY
          cat > scripts/xtekct_to_json.py << 'PY'
{{XTEKCT_TO_JSON_PY}}
PY
          cat > scripts/aggregate_json.py << 'PY'
{{AGGREGATE_JSON_PY}}
PY

      - name: Determine changed source files
        id: changed
        shell: bash
        run: |
          set -euo pipefail
          BEFORE="${{ github.event.before }}"
          if [ -z "${BEFORE}" ] || [ "${BEFORE}" = "0000000000000000000000000000000000000000" ]; then
            # First push / branch create: parse all non-JSON files under data/**
            files=$(git ls-files 'data/**' | grep -v -E '\.json$' || true)
          else
            # Only parse changed non-JSON files under data/**
            files=$(git diff --name-only "${BEFORE}" "${{ github.sha }}" -- 'data/**' | grep -v -E '\.json$' || true)
          fi
          echo "Changed files:"
          printf '%s\n' "$files"
          echo "FILES=$(printf '%s\n' "$files" | paste -sd "," -)" >> $GITHUB_ENV

      - name: Parse changed files â†’ JSON (and move originals to data/completed)
        if: env.FILES != ''
        shell: bash
        run: |
          set -euo pipefail
          IFS=',' read -ra arr <<< "$FILES"
          for f in "${arr[@]}"; do
            [ -z "$f" ] && continue
            [ ! -f "$f" ] && continue
            # parse_any.py decides the correct parser based on extension
            python scripts/parse_any.py "$f" -o data/parsed --completed-dir data/completed --pretty
            # Safety: in case an older parser ever copied instead of moved
            if [ -f "$f" ]; then
              echo "Removing original leftover: $f"
              rm -f "$f"
            fi
          done

      - name: Aggregate & de-duplicate metadata (append cumulatively)
        shell: bash
        run: |
          set -euo pipefail
          python scripts/aggregate_json.py data/metadata.json

      - name: Commit & push results
        shell: bash
        run: |
          set -euo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi
          git commit -m "Parse, move originals to completed, and aggregate metadata [skip ci]"
          git push
